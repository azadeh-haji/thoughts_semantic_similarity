{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fresh-promise",
   "metadata": {},
   "source": [
    "#### read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "absolute-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read full data in\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Though listing and Regulation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "american-overhead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Status</th>\n",
       "      <th>IPAddress</th>\n",
       "      <th>Progress</th>\n",
       "      <th>Duration (in seconds)</th>\n",
       "      <th>Finished</th>\n",
       "      <th>RecordedDate</th>\n",
       "      <th>ResponseId</th>\n",
       "      <th>RecipientLastName</th>\n",
       "      <th>...</th>\n",
       "      <th>20_healthRT_Click Count</th>\n",
       "      <th>debrief1</th>\n",
       "      <th>debrief2</th>\n",
       "      <th>consent2</th>\n",
       "      <th>SC0</th>\n",
       "      <th>SC1</th>\n",
       "      <th>SC2</th>\n",
       "      <th>SC3</th>\n",
       "      <th>id</th>\n",
       "      <th>Condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Start Date</td>\n",
       "      <td>End Date</td>\n",
       "      <td>Response Type</td>\n",
       "      <td>IP Address</td>\n",
       "      <td>Progress</td>\n",
       "      <td>Duration (in seconds)</td>\n",
       "      <td>Finished</td>\n",
       "      <td>Recorded Date</td>\n",
       "      <td>Response ID</td>\n",
       "      <td>Recipient Last Name</td>\n",
       "      <td>...</td>\n",
       "      <td>Timing - https://utorontopsych.az1.qualtrics.c...</td>\n",
       "      <td>What do you think the purpose of this study was?</td>\n",
       "      <td>Did you notice anything odd about the study? W...</td>\n",
       "      <td>Participant Consent: My signature indicates I ...</td>\n",
       "      <td>English_Test</td>\n",
       "      <td>Reasons_Test</td>\n",
       "      <td>Reg_Test</td>\n",
       "      <td>Native_Eng</td>\n",
       "      <td>id</td>\n",
       "      <td>Condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"ImportId\":\"startDate\",\"timeZone\":\"America/De...</td>\n",
       "      <td>{\"ImportId\":\"endDate\",\"timeZone\":\"America/Denv...</td>\n",
       "      <td>{\"ImportId\":\"status\"}</td>\n",
       "      <td>{\"ImportId\":\"ipAddress\"}</td>\n",
       "      <td>{\"ImportId\":\"progress\"}</td>\n",
       "      <td>{\"ImportId\":\"duration\"}</td>\n",
       "      <td>{\"ImportId\":\"finished\"}</td>\n",
       "      <td>{\"ImportId\":\"recordedDate\",\"timeZone\":\"America...</td>\n",
       "      <td>{\"ImportId\":\"_recordId\"}</td>\n",
       "      <td>{\"ImportId\":\"recipientLastName\"}</td>\n",
       "      <td>...</td>\n",
       "      <td>{\"ImportId\":\"20_QID171_CLICK_COUNT\"}</td>\n",
       "      <td>{\"ImportId\":\"QID113_TEXT\"}</td>\n",
       "      <td>{\"ImportId\":\"QID114_TEXT\"}</td>\n",
       "      <td>{\"ImportId\":\"QID95\"}</td>\n",
       "      <td>{\"ImportId\":\"SC_3kq6pM8K2KXJ3UN\"}</td>\n",
       "      <td>{\"ImportId\":\"SC_8AkbAZDb1qk88WF\"}</td>\n",
       "      <td>{\"ImportId\":\"SC_6fHiZnwJgVKQ6O1\"}</td>\n",
       "      <td>{\"ImportId\":\"SC_cYe6UJrN9QdqQQJ\"}</td>\n",
       "      <td>{\"ImportId\":\"id\"}</td>\n",
       "      <td>{\"ImportId\":\"Condition\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-28 12:48</td>\n",
       "      <td>2019-11-28 13:10</td>\n",
       "      <td>IP Address</td>\n",
       "      <td>98.247.142.31</td>\n",
       "      <td>100</td>\n",
       "      <td>1326</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>2019-11-28 13:10</td>\n",
       "      <td>R_2Yf9NqzNdvMSCnH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Gauge influence of healthy eating reminders on...</td>\n",
       "      <td>nope!</td>\n",
       "      <td>I agree</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>49672855</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-28 12:39</td>\n",
       "      <td>2019-11-28 13:10</td>\n",
       "      <td>IP Address</td>\n",
       "      <td>99.111.130.223</td>\n",
       "      <td>100</td>\n",
       "      <td>1864</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>2019-11-28 13:10</td>\n",
       "      <td>R_1d6mWtvT33pT6K5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>To see how people feel about certain foods bef...</td>\n",
       "      <td>I did not notice anything odd</td>\n",
       "      <td>I agree</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38501484</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-28 12:38</td>\n",
       "      <td>2019-11-28 13:11</td>\n",
       "      <td>IP Address</td>\n",
       "      <td>71.219.142.102</td>\n",
       "      <td>100</td>\n",
       "      <td>1972</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>2019-11-28 13:11</td>\n",
       "      <td>R_2dEfOsx1KRXrUs0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>DETERMINE MY CHOICES IN FOOD</td>\n",
       "      <td>NONE</td>\n",
       "      <td>I agree</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24539356</td>\n",
       "      <td>Natural</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           StartDate  \\\n",
       "0                                         Start Date   \n",
       "1  {\"ImportId\":\"startDate\",\"timeZone\":\"America/De...   \n",
       "2                                   2019-11-28 12:48   \n",
       "3                                   2019-11-28 12:39   \n",
       "4                                   2019-11-28 12:38   \n",
       "\n",
       "                                             EndDate                 Status  \\\n",
       "0                                           End Date          Response Type   \n",
       "1  {\"ImportId\":\"endDate\",\"timeZone\":\"America/Denv...  {\"ImportId\":\"status\"}   \n",
       "2                                   2019-11-28 13:10             IP Address   \n",
       "3                                   2019-11-28 13:10             IP Address   \n",
       "4                                   2019-11-28 13:11             IP Address   \n",
       "\n",
       "                  IPAddress                 Progress    Duration (in seconds)  \\\n",
       "0                IP Address                 Progress    Duration (in seconds)   \n",
       "1  {\"ImportId\":\"ipAddress\"}  {\"ImportId\":\"progress\"}  {\"ImportId\":\"duration\"}   \n",
       "2             98.247.142.31                      100                     1326   \n",
       "3            99.111.130.223                      100                     1864   \n",
       "4            71.219.142.102                      100                     1972   \n",
       "\n",
       "                  Finished                                       RecordedDate  \\\n",
       "0                 Finished                                      Recorded Date   \n",
       "1  {\"ImportId\":\"finished\"}  {\"ImportId\":\"recordedDate\",\"timeZone\":\"America...   \n",
       "2                     TRUE                                   2019-11-28 13:10   \n",
       "3                     TRUE                                   2019-11-28 13:10   \n",
       "4                     TRUE                                   2019-11-28 13:11   \n",
       "\n",
       "                 ResponseId                 RecipientLastName  ...  \\\n",
       "0               Response ID               Recipient Last Name  ...   \n",
       "1  {\"ImportId\":\"_recordId\"}  {\"ImportId\":\"recipientLastName\"}  ...   \n",
       "2         R_2Yf9NqzNdvMSCnH                               NaN  ...   \n",
       "3         R_1d6mWtvT33pT6K5                               NaN  ...   \n",
       "4         R_2dEfOsx1KRXrUs0                               NaN  ...   \n",
       "\n",
       "                             20_healthRT_Click Count  \\\n",
       "0  Timing - https://utorontopsych.az1.qualtrics.c...   \n",
       "1               {\"ImportId\":\"20_QID171_CLICK_COUNT\"}   \n",
       "2                                                  1   \n",
       "3                                                  1   \n",
       "4                                                  1   \n",
       "\n",
       "                                            debrief1  \\\n",
       "0   What do you think the purpose of this study was?   \n",
       "1                         {\"ImportId\":\"QID113_TEXT\"}   \n",
       "2  Gauge influence of healthy eating reminders on...   \n",
       "3  To see how people feel about certain foods bef...   \n",
       "4                       DETERMINE MY CHOICES IN FOOD   \n",
       "\n",
       "                                            debrief2  \\\n",
       "0  Did you notice anything odd about the study? W...   \n",
       "1                         {\"ImportId\":\"QID114_TEXT\"}   \n",
       "2                                              nope!   \n",
       "3                      I did not notice anything odd   \n",
       "4                                               NONE   \n",
       "\n",
       "                                            consent2  \\\n",
       "0  Participant Consent: My signature indicates I ...   \n",
       "1                               {\"ImportId\":\"QID95\"}   \n",
       "2                                            I agree   \n",
       "3                                            I agree   \n",
       "4                                            I agree   \n",
       "\n",
       "                                 SC0                                SC1  \\\n",
       "0                       English_Test                       Reasons_Test   \n",
       "1  {\"ImportId\":\"SC_3kq6pM8K2KXJ3UN\"}  {\"ImportId\":\"SC_8AkbAZDb1qk88WF\"}   \n",
       "2                                  5                                  3   \n",
       "3                                  5                                  2   \n",
       "4                                  3                                  2   \n",
       "\n",
       "                                 SC2                                SC3  \\\n",
       "0                           Reg_Test                         Native_Eng   \n",
       "1  {\"ImportId\":\"SC_6fHiZnwJgVKQ6O1\"}  {\"ImportId\":\"SC_cYe6UJrN9QdqQQJ\"}   \n",
       "2                                  2                                  1   \n",
       "3                                  2                                  1   \n",
       "4                                  1                                  1   \n",
       "\n",
       "                  id                 Condition  \n",
       "0                 id                 Condition  \n",
       "1  {\"ImportId\":\"id\"}  {\"ImportId\":\"Condition\"}  \n",
       "2           49672855                    Health  \n",
       "3           38501484                    Health  \n",
       "4           24539356                   Natural  \n",
       "\n",
       "[5 rows x 1197 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "indie-fellowship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273, 1197)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Finished']!='TRUE']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-witch",
   "metadata": {},
   "source": [
    "#### select data for processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cellular-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of columns to process\n",
    "columnsToUse = ['Progress','gender','location','ethnicity','age','healthyEating_1','dietaryRestriction',\n",
    "                'foodAllergies','Native English Speak','Eng. Q1','Eng. Q2','Eng. Q3','Eng. Q4','Eng. Q5',\n",
    "                'Eng. Q6','hunger_1','Condition']\n",
    "#fill in 20 item colums\n",
    "for n in range(1,21):\n",
    "    columnsToUse.append(str(n)+'_choicePre')\n",
    "    columnsToUse.append(str(n)+'_choicePost')\n",
    "    columnsToUse.append(str(n)+'_ChoiceTask_Resp')\n",
    "    columnsToUse.append(str(n)+'_Reason1')\n",
    "    columnsToUse.append(str(n)+'_Reason2')\n",
    "    columnsToUse.append(str(n)+'_Reason3')\n",
    "    columnsToUse.append(str(n)+'_Reason4')\n",
    "    columnsToUse.append(str(n)+'_taste')\n",
    "    columnsToUse.append(str(n)+'_health')\n",
    "    \n",
    "columnsToUse.append('debrief1')\n",
    "columnsToUse.append('debrief2')\n",
    "# columnsToUse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "enormous-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe with wanted columns and rows\n",
    "dfInUse = df[columnsToUse].copy()\n",
    "dfInUse.drop(axis=0, index=[0,1],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "figured-parameter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Progress</th>\n",
       "      <th>gender</th>\n",
       "      <th>location</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>age</th>\n",
       "      <th>healthyEating_1</th>\n",
       "      <th>dietaryRestriction</th>\n",
       "      <th>foodAllergies</th>\n",
       "      <th>Native English Speak</th>\n",
       "      <th>Eng. Q1</th>\n",
       "      <th>...</th>\n",
       "      <th>20_choicePost</th>\n",
       "      <th>20_ChoiceTask_Resp</th>\n",
       "      <th>20_Reason1</th>\n",
       "      <th>20_Reason2</th>\n",
       "      <th>20_Reason3</th>\n",
       "      <th>20_Reason4</th>\n",
       "      <th>20_taste</th>\n",
       "      <th>20_health</th>\n",
       "      <th>debrief1</th>\n",
       "      <th>debrief2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>Female</td>\n",
       "      <td>United States</td>\n",
       "      <td>White/Caucasian</td>\n",
       "      <td>47</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I am a native English speaker</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>...</td>\n",
       "      <td>Dislike</td>\n",
       "      <td>No, I would not like to eat this</td>\n",
       "      <td>I'd rather have real fruit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Slightly tasty</td>\n",
       "      <td>Slightly unhealthy</td>\n",
       "      <td>Gauge influence of healthy eating reminders on...</td>\n",
       "      <td>nope!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>Male</td>\n",
       "      <td>United States</td>\n",
       "      <td>White/Caucasian</td>\n",
       "      <td>21</td>\n",
       "      <td>83</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>I am a native English speaker</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>...</td>\n",
       "      <td>Dislike</td>\n",
       "      <td>No, I would not like to eat this</td>\n",
       "      <td>High sugar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Slightly tasty</td>\n",
       "      <td>Very healthy</td>\n",
       "      <td>To see how people feel about certain foods bef...</td>\n",
       "      <td>I did not notice anything odd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>Female</td>\n",
       "      <td>United States</td>\n",
       "      <td>White/Caucasian</td>\n",
       "      <td>48</td>\n",
       "      <td>81</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>I am a native English speaker</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>...</td>\n",
       "      <td>Like</td>\n",
       "      <td>Yes, I would like to eat this</td>\n",
       "      <td>THESE ARE SWEET AND CHEWY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Very tasty</td>\n",
       "      <td>Very unhealthy</td>\n",
       "      <td>DETERMINE MY CHOICES IN FOOD</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>Male</td>\n",
       "      <td>United States</td>\n",
       "      <td>White/Caucasian</td>\n",
       "      <td>28</td>\n",
       "      <td>99</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>I am a native English speaker</td>\n",
       "      <td>Correct</td>\n",
       "      <td>...</td>\n",
       "      <td>Strong like</td>\n",
       "      <td>Yes, I would like to eat this</td>\n",
       "      <td>it was good for health</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Slightly tasty</td>\n",
       "      <td>Neither healthy nor unhealthy.</td>\n",
       "      <td>know about the food taste and health.</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>Male</td>\n",
       "      <td>Canada</td>\n",
       "      <td>White/Caucasian</td>\n",
       "      <td>27</td>\n",
       "      <td>64</td>\n",
       "      <td>None.</td>\n",
       "      <td>None.</td>\n",
       "      <td>I am not a native English speaker</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>...</td>\n",
       "      <td>Strong like</td>\n",
       "      <td>Yes, I would like to eat this</td>\n",
       "      <td>Fruits snacks aren't healthy.</td>\n",
       "      <td>Fruit snacks are tasty.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Slightly tasty</td>\n",
       "      <td>Slightly unhealthy</td>\n",
       "      <td>Understand how people chose what they want to ...</td>\n",
       "      <td>I did not notice anything odd (other than the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>38</td>\n",
       "      <td>Male</td>\n",
       "      <td>United States</td>\n",
       "      <td>African Canadian/Black</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I am a native English speaker</td>\n",
       "      <td>Correct</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>12</td>\n",
       "      <td>Female</td>\n",
       "      <td>United States</td>\n",
       "      <td>African Canadian/Black</td>\n",
       "      <td>31</td>\n",
       "      <td>76</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>I am a native English speaker</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>13</td>\n",
       "      <td>Female</td>\n",
       "      <td>United States</td>\n",
       "      <td>White/Caucasian</td>\n",
       "      <td>50</td>\n",
       "      <td>61</td>\n",
       "      <td>n0</td>\n",
       "      <td>no</td>\n",
       "      <td>I am a native English speaker</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>73</td>\n",
       "      <td>Female</td>\n",
       "      <td>United States</td>\n",
       "      <td>White/Caucasian</td>\n",
       "      <td>39</td>\n",
       "      <td>72</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>I am a native English speaker</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes, I would like to eat this</td>\n",
       "      <td>These are healthy and delicious</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>93</td>\n",
       "      <td>Male</td>\n",
       "      <td>United States</td>\n",
       "      <td>White/Caucasian</td>\n",
       "      <td>72</td>\n",
       "      <td>66</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>I am a native English speaker</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>...</td>\n",
       "      <td>Strong dislike</td>\n",
       "      <td>No, I would not like to eat this</td>\n",
       "      <td>Not enthralled about these. I'll pass.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>271 rows × 199 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Progress  gender       location               ethnicity age  \\\n",
       "2        100  Female  United States         White/Caucasian  47   \n",
       "3        100    Male  United States         White/Caucasian  21   \n",
       "4        100  Female  United States         White/Caucasian  48   \n",
       "5        100    Male  United States         White/Caucasian  28   \n",
       "6        100    Male         Canada         White/Caucasian  27   \n",
       "..       ...     ...            ...                     ...  ..   \n",
       "268       38    Male  United States  African Canadian/Black  25   \n",
       "269       12  Female  United States  African Canadian/Black  31   \n",
       "270       13  Female  United States         White/Caucasian  50   \n",
       "271       73  Female  United States         White/Caucasian  39   \n",
       "272       93    Male  United States         White/Caucasian  72   \n",
       "\n",
       "    healthyEating_1 dietaryRestriction foodAllergies  \\\n",
       "2                38                NaN           NaN   \n",
       "3                83               None          None   \n",
       "4                81               NONE          NONE   \n",
       "5                99               none          none   \n",
       "6                64              None.         None.   \n",
       "..              ...                ...           ...   \n",
       "268             100                NaN           NaN   \n",
       "269              76               none          none   \n",
       "270              61                 n0            no   \n",
       "271              72                 no            no   \n",
       "272              66                 no            no   \n",
       "\n",
       "                  Native English Speak    Eng. Q1  ...   20_choicePost  \\\n",
       "2        I am a native English speaker  Incorrect  ...         Dislike   \n",
       "3        I am a native English speaker  Incorrect  ...         Dislike   \n",
       "4        I am a native English speaker  Incorrect  ...            Like   \n",
       "5        I am a native English speaker    Correct  ...     Strong like   \n",
       "6    I am not a native English speaker  Incorrect  ...     Strong like   \n",
       "..                                 ...        ...  ...             ...   \n",
       "268      I am a native English speaker    Correct  ...             NaN   \n",
       "269      I am a native English speaker  Incorrect  ...             NaN   \n",
       "270      I am a native English speaker  Incorrect  ...             NaN   \n",
       "271      I am a native English speaker  Incorrect  ...             NaN   \n",
       "272      I am a native English speaker  Incorrect  ...  Strong dislike   \n",
       "\n",
       "                   20_ChoiceTask_Resp                              20_Reason1  \\\n",
       "2    No, I would not like to eat this              I'd rather have real fruit   \n",
       "3    No, I would not like to eat this                              High sugar   \n",
       "4       Yes, I would like to eat this               THESE ARE SWEET AND CHEWY   \n",
       "5       Yes, I would like to eat this                  it was good for health   \n",
       "6       Yes, I would like to eat this           Fruits snacks aren't healthy.   \n",
       "..                                ...                                     ...   \n",
       "268                               NaN                                     NaN   \n",
       "269                               NaN                                     NaN   \n",
       "270                               NaN                                     NaN   \n",
       "271     Yes, I would like to eat this         These are healthy and delicious   \n",
       "272  No, I would not like to eat this  Not enthralled about these. I'll pass.   \n",
       "\n",
       "                  20_Reason2 20_Reason3 20_Reason4        20_taste  \\\n",
       "2                        NaN        NaN        NaN  Slightly tasty   \n",
       "3                        NaN        NaN        NaN  Slightly tasty   \n",
       "4                        NaN        NaN        NaN      Very tasty   \n",
       "5                        NaN        NaN        NaN  Slightly tasty   \n",
       "6    Fruit snacks are tasty.        NaN        NaN  Slightly tasty   \n",
       "..                       ...        ...        ...             ...   \n",
       "268                      NaN        NaN        NaN             NaN   \n",
       "269                      NaN        NaN        NaN             NaN   \n",
       "270                      NaN        NaN        NaN             NaN   \n",
       "271                      NaN        NaN        NaN             NaN   \n",
       "272                      NaN        NaN        NaN             NaN   \n",
       "\n",
       "                          20_health  \\\n",
       "2                Slightly unhealthy   \n",
       "3                      Very healthy   \n",
       "4                    Very unhealthy   \n",
       "5    Neither healthy nor unhealthy.   \n",
       "6                Slightly unhealthy   \n",
       "..                              ...   \n",
       "268                             NaN   \n",
       "269                             NaN   \n",
       "270                             NaN   \n",
       "271                             NaN   \n",
       "272                             NaN   \n",
       "\n",
       "                                              debrief1  \\\n",
       "2    Gauge influence of healthy eating reminders on...   \n",
       "3    To see how people feel about certain foods bef...   \n",
       "4                         DETERMINE MY CHOICES IN FOOD   \n",
       "5                know about the food taste and health.   \n",
       "6    Understand how people chose what they want to ...   \n",
       "..                                                 ...   \n",
       "268                                                NaN   \n",
       "269                                                NaN   \n",
       "270                                                NaN   \n",
       "271                                                NaN   \n",
       "272                                                NaN   \n",
       "\n",
       "                                              debrief2  \n",
       "2                                                nope!  \n",
       "3                        I did not notice anything odd  \n",
       "4                                                 NONE  \n",
       "5                                                 none  \n",
       "6    I did not notice anything odd (other than the ...  \n",
       "..                                                 ...  \n",
       "268                                                NaN  \n",
       "269                                                NaN  \n",
       "270                                                NaN  \n",
       "271                                                NaN  \n",
       "272                                                NaN  \n",
       "\n",
       "[271 rows x 199 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfInUse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "italian-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# break into 2 dataframes for Natural and Health conditions\n",
    "df_Nat    = dfInUse[dfInUse['Condition']=='Natural']\n",
    "df_Health = dfInUse[dfInUse['Condition']=='Health']\n",
    "# df_Nat.shape\n",
    "\n",
    "# extraxt all reasons into a list\n",
    "reasons_nat    = list()\n",
    "reasons_health = list()\n",
    "\n",
    "for s in range(df_Nat.shape[0]):\n",
    "    for n in range(1,21):\n",
    "        reasons_nat.append(df_Nat[str(n)+'_Reason1'].iloc[s])\n",
    "        reasons_nat.append(df_Nat[str(n)+'_Reason2'].iloc[s])\n",
    "        reasons_nat.append(df_Nat[str(n)+'_Reason3'].iloc[s])\n",
    "        reasons_nat.append(df_Nat[str(n)+'_Reason4'].iloc[s])\n",
    "        \n",
    "for s in range(df_Health.shape[0]):\n",
    "    for n in range(1,21):\n",
    "        reasons_health.append(df_Health[str(n)+'_Reason1'].iloc[s])\n",
    "        reasons_health.append(df_Health[str(n)+'_Reason2'].iloc[s])\n",
    "        reasons_health.append(df_Health[str(n)+'_Reason3'].iloc[s])\n",
    "        reasons_health.append(df_Health[str(n)+'_Reason4'].iloc[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "supreme-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reasons_nat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "excessive-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid on nan entries\n",
    "from itertools import compress\n",
    "\n",
    "reasons_nat    = list(compress(reasons_nat, pd.notna(reasons_nat)))\n",
    "reasons_health = list(compress(reasons_health, pd.notna(reasons_health)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ordinary-glory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  pd.notna(reasons_health)\n",
    "# # pd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-illinois",
   "metadata": {},
   "source": [
    "### Topic modeling using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-technical",
   "metadata": {},
   "source": [
    "#### separate models for Natural and Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baking-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train the count vectorizer ; \n",
    "# this creates a sparse matrix of document x word containing how many times each word was repeated in a document(reason)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv_nat     = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "cv_health  = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm_nat    = cv_nat.fit_transform(reasons_nat)\n",
    "dtm_health = cv_health.fit_transform(reasons_health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "challenging-courage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2884x552 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7643 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_nat # reason X words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "interim-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train the LDA model\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "LDA_nat    = LatentDirichletAllocation(n_components=3,random_state=42) # choose how many topics (2 to 5)\n",
    "LDA_health = LatentDirichletAllocation(n_components=3,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "built-plane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=3, random_state=42)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_nat.fit(dtm_nat)\n",
    "LDA_health.fit(dtm_health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "nominated-collection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "555\n"
     ]
    }
   ],
   "source": [
    "print(len(cv_nat.get_feature_names())) # features are words\n",
    "print(len(cv_health.get_feature_names())) # features are words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "naval-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cv_nat.get_feature_names())\n",
    "# print(cv_health.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "permanent-stewart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LDA_nat.components_) # number of topics (predefined by user when training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "strange-negotiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 46 125 491 186 153 293 152 475 483 281]\n",
      "[282 185 279 163  69  34 180 223 472 264]\n"
     ]
    }
   ],
   "source": [
    "print(LDA_nat.components_[0].argsort()[-10:]) # list of word indicies with highest occurences in each topic\n",
    "print(LDA_health.components_[0].argsort()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "optimum-insurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bland\n",
      "crunchy\n",
      "texture\n",
      "flavor\n",
      "dont\n",
      "love\n",
      "don\n",
      "sweet\n",
      "taste\n",
      "like\n",
      "\n",
      "\n",
      "low\n",
      "fruit\n",
      "lot\n",
      "fat\n",
      "calories\n",
      "bad\n",
      "food\n",
      "high\n",
      "sugar\n",
      "like\n"
     ]
    }
   ],
   "source": [
    "# display words whose indicies are displayed above\n",
    "for index in LDA_nat.components_[0].argsort()[-10:]:\n",
    "    print(cv_nat.get_feature_names()[index])\n",
    "print('\\n')\n",
    "for index in LDA_health.components_[0].argsort()[-10:]:\n",
    "    print(cv_health.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aggressive-technician",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 WORDS FOR TOPIC #0 IN NATURAL ARE:\n",
      "like 462.11| taste 368.8| sweet 151.29| don 137.29| love 75.61| dont 75.3| flavor 68.29| texture 68.11| crunchy 61.68| bland 57.28| \n",
      "\n",
      "\n",
      "TOP 10 WORDS FOR TOPIC #1 IN NATURAL ARE:\n",
      "eat 194.87| food 140.31| like 125.23| looks 84.28| chocolate 75.27| love 62.99| cream 62.31| ice 54.32| unhealthy 54.3| bad 47.27| \n",
      "\n",
      "\n",
      "TOP 10 WORDS FOR TOPIC #2 IN NATURAL ARE:\n",
      "good 416.46| healthy 261.31| tasty 128.24| tastes 95.29| love 71.4| delicious 68.04| snack 58.71| great 46.64| green 39.29| hate 37.15| \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TOP 10 WORDS FOR TOPIC #0 IN HEALTH ARE:\n",
      "like 418.99| sugar 352.31| high 213.9| food 125.75| bad 97.15| calories 88.3| fat 81.31| lot 80.34| fruit 62.27| low 42.88| \n",
      "\n",
      "\n",
      "TOP 10 WORDS FOR TOPIC #1 IN HEALTH ARE:\n",
      "tasty 201.31| eat 197.41| unhealthy 92.72| love 87.03| chocolate 61.06| fiber 53.28| crunchy 43.31| processed 43.3| looks 37.29| favorite 32.31| \n",
      "\n",
      "\n",
      "TOP 10 WORDS FOR TOPIC #2 IN HEALTH ARE:\n",
      "healthy 569.13| good 493.82| taste 241.96| tastes 127.23| sweet 109.3| delicious 95.29| vitamins 79.32| don 77.86| snack 65.0| vegetable 59.48| \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list words with highest occurence in each topic\n",
    "for index,topic in enumerate(LDA_nat.components_):\n",
    "    print(f'TOP 10 WORDS FOR TOPIC #{index} IN NATURAL ARE:')\n",
    "    print(''.join([cv_nat.get_feature_names()[ind]+' '+ str(round(topic[ind], 2)) +'| ' for ind in topic.argsort()[:-11:-1]]))\n",
    "    print('\\n')\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "for index,topic in enumerate(LDA_health.components_):\n",
    "    print(f'TOP 10 WORDS FOR TOPIC #{index} IN HEALTH ARE:')\n",
    "    print(''.join([cv_health.get_feature_names()[ind]+' '+ str(round(topic[ind], 2)) +'| ' for ind in topic.argsort()[:-11:-1]]))\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "parliamentary-white",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 WORDS FOR TOPIC #0 IN HEALTH ARE:\n",
      "low 0.01| fruit 0.02| lot 0.03| fat 0.03| calories 0.03| bad 0.03| food 0.04| high 0.07| sugar 0.12| like 0.14| \n",
      "\n",
      "\n",
      "TOP 10 WORDS FOR TOPIC #1 IN HEALTH ARE:\n",
      "favorite 0.01| looks 0.01| processed 0.02| crunchy 0.02| fiber 0.02| chocolate 0.02| love 0.03| unhealthy 0.04| eat 0.08| tasty 0.08| \n",
      "\n",
      "\n",
      "TOP 10 WORDS FOR TOPIC #2 IN HEALTH ARE:\n",
      "vegetable 0.02| snack 0.02| don 0.02| vitamins 0.02| delicious 0.03| sweet 0.03| tastes 0.03| taste 0.07| good 0.13| healthy 0.16| \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,topic in enumerate(LDA_health.exp_dirichlet_component_):\n",
    "    print(f'TOP 10 WORDS FOR TOPIC #{index} IN HEALTH ARE:')\n",
    "    print(''.join([cv_health.get_feature_names()[ind]+' '+ str(round(topic[ind], 2)) +'| ' for ind in topic.argsort()[-10:]]))\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dress-congo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 552 artists>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.bar(range(LDA_nat.components_.shape[1]),LDA_nat.exp_dirichlet_component_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-screening",
   "metadata": {},
   "source": [
    "#### measure similarity of the first 10 words in each topic with taste and health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "unauthorized-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "steady-marks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5473916200990734"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u'taste').similarity(nlp(u'sugar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "happy-cowboy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196.40476040172896"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_nat.components_[1][LDA_nat.components_[1].argmax()]\n",
    "# cv_nat.get_feature_names()[LDA_nat.components_[0].argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "institutional-slovenia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507.803680265623"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_health.get_feature_names()[LDA_health.components_[0].argmax()]\n",
    "LDA_health.components_[1][LDA_health.components_[1].argmax()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dying-mercury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall similarity of topic #1 to taste in Natural condition: 0.54\n",
      "overall similarity of topic #1 to health in Natural condition: 0.18\n",
      "overall similarity of topic #1 to taste in Health condition: 0.43\n",
      "overall similarity of topic #1 to health in Health condition: 0.34\n"
     ]
    }
   ],
   "source": [
    "##get similarity with taste and health\n",
    "import numpy as np\n",
    "simil_nat_taste = np.full((5,10),np.nan)\n",
    "for index,topic in enumerate(LDA_nat.components_):\n",
    "    simil_nat_taste[index,:] = np.asarray([nlp('taste').similarity(nlp(cv_nat.get_feature_names()[word])) for word in topic.argsort()[:-11:-1]])\n",
    "print('overall similarity of topic #1 to taste in Natural condition: '+str(round(simil_nat_taste[0,:].mean(),2)))\n",
    "\n",
    "simil_nat_health = np.full((5,10),np.nan)\n",
    "for index,topic in enumerate(LDA_nat.components_):\n",
    "    simil_nat_health[index,:] = np.asarray([nlp('health').similarity(nlp(cv_nat.get_feature_names()[word])) for word in topic.argsort()[:-11:-1]])\n",
    "print('overall similarity of topic #1 to health in Natural condition: '+str(round(simil_nat_health[0,:].mean(),2)))\n",
    "\n",
    "\n",
    "simil_health_taste = np.full((5,10),np.nan)\n",
    "for index,topic in enumerate(LDA_health.components_):\n",
    "    simil_health_taste[index,:] = np.asarray([nlp('taste').similarity(nlp(cv_health.get_feature_names()[word])) for word in topic.argsort()[:-11:-1]])\n",
    "print('overall similarity of topic #1 to taste in Health condition: '+str(round(simil_health_taste[0,:].mean(),2)))\n",
    "\n",
    "simil_health_health = np.full((5,10),np.nan)\n",
    "for index,topic in enumerate(LDA_health.components_):\n",
    "    simil_health_health[index,:] = np.asarray([nlp('health').similarity(nlp(cv_health.get_feature_names()[word])) for word in topic.argsort()[:-11:-1]])\n",
    "print('overall similarity of topic #1 to health in Health condition: '+str(round(simil_health_health[0,:].mean(),2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "romantic-korean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45390466168900717"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = LDA_nat.components_[0]\n",
    "nlp('taste').similarity(nlp(cv_nat.get_feature_names()[topic.argsort()[-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-calcium",
   "metadata": {},
   "source": [
    "#### one model for both conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "amateur-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "allreasons = list()\n",
    "for s in range(dfInUse.shape[0]):\n",
    "    for n in range(1,21):\n",
    "        allreasons.append(dfInUse[str(n)+'_Reason1'].iloc[s])\n",
    "        allreasons.append(dfInUse[str(n)+'_Reason2'].iloc[s])\n",
    "        allreasons.append(dfInUse[str(n)+'_Reason3'].iloc[s])\n",
    "        allreasons.append(dfInUse[str(n)+'_Reason4'].iloc[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "french-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "allreasons    = list(compress(allreasons, pd.notna(allreasons)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "stone-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv_all     = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm_all    = cv_all.fit_transform(allreasons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "baking-celebration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "834"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_all.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "binary-corner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=3, random_state=42)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_all\n",
    "LDA_all    = LatentDirichletAllocation(n_components=3,random_state=42)\n",
    "LDA_all.fit(dtm_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "distant-spiritual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 WORDS FOR TOPIC #1 ARE:\n",
      "healthy 767.93| sugar 389.3| eat 376.55| like 347.49| love 307.47| high 233.01| don 231.9| food 192.04| taste 172.78| bad 151.3| \n",
      "\n",
      "\n",
      "TOP 10 WORDS FOR TOPIC #2 ARE:\n",
      "like 658.85| tasty 329.31| sweet 260.3| delicious 181.08| unhealthy 163.29| looks 121.31| crunchy 109.29| dont 104.32| chocolate 101.81| flavor 98.84| \n",
      "\n",
      "\n",
      "TOP 10 WORDS FOR TOPIC #3 ARE:\n",
      "good 961.29| taste 443.24| tastes 222.3| great 152.28| green 95.31| vitamins 94.11| favorite 92.98| vegetable 86.29| health 78.75| apples 66.58| \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,topic in enumerate(LDA_all.components_):\n",
    "    print(f'TOP 10 WORDS FOR TOPIC #{index+1} ARE:')\n",
    "    print(''.join([cv_all.get_feature_names()[ind]+' '+ str(round(topic[ind], 2)) +'| ' for ind in topic.argsort()[:-11:-1]]))\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "approved-payroll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall similarity of topic #1 to taste: 0.54\n",
      "\n",
      "\n",
      "overall similarity of topic #2 to taste: 0.47\n",
      "\n",
      "\n",
      "overall similarity of topic #3 to taste: 0.53\n",
      "\n",
      "\n",
      "overall similarity of topic #1 to health: 0.18\n",
      "\n",
      "\n",
      "overall similarity of topic #2 to health: 0.29\n",
      "\n",
      "\n",
      "overall similarity of topic #3 to health: 0.28\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simil_all_taste = np.full((5,10),np.nan)\n",
    "for index,topic in enumerate(LDA_all.components_):\n",
    "    simil_all_taste[index,:] = np.asarray([nlp('taste').similarity(nlp(cv_all.get_feature_names()[word])) for word in topic.argsort()[:-11:-1]])\n",
    "    print(f'overall similarity of topic #{index+1} to taste: '+str(round(simil_nat_taste[index,:].mean(),2)))\n",
    "    print('\\n')\n",
    "\n",
    "simil_all_health = np.full((5,10),np.nan)\n",
    "for index,topic in enumerate(LDA_all.components_):\n",
    "    simil_all_health[index,:] = np.asarray([nlp('health').similarity(nlp(cv_all.get_feature_names()[word])) for word in topic.argsort()[:-11:-1]])\n",
    "    print(f'overall similarity of topic #{index+1} to health: '+str(round(simil_nat_health[index,:].mean(),2)))\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-grade",
   "metadata": {},
   "source": [
    "### Classification based on condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-drive",
   "metadata": {},
   "source": [
    "#### each word feature as an observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "paperback-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(cv_nat.get_feature_names())+len(cv_health.get_feature_names())\n",
    "n_words_nat = len(cv_nat.get_feature_names())\n",
    "X = np.full((n_words,300),np.nan) # 300 is spacy word vector length\n",
    "y = np.full((n_words),np.nan)\n",
    "for index,word in enumerate(cv_nat.get_feature_names()):\n",
    "    X[index,:] = nlp(word).vector\n",
    "    y[index]   = 0\n",
    "for index,word in enumerate(cv_health.get_feature_names()):\n",
    "    X[index+n_words_nat,:] = nlp(word).vector\n",
    "    y[index+n_words_nat]   = 1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "balanced-reunion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.34      0.37      0.36       132\n",
      "         1.0       0.38      0.34      0.36       145\n",
      "\n",
      "    accuracy                           0.36       277\n",
      "   macro avg       0.36      0.36      0.36       277\n",
      "weighted avg       0.36      0.36      0.36       277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25)\n",
    "\n",
    "svm = LinearSVC(max_iter=5000)\n",
    "svm.fit(X_train,y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# scores = cross_val_score(clf,X,y,cv = 6)\n",
    "# print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "conservative-sigma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.53513514 0.48108108 0.41621622 0.48913043 0.45652174 0.51630435]\n",
      "0.4823981590285938\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(svm,X,y,cv=6)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-round",
   "metadata": {},
   "source": [
    "#### classify only based on unique words in each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "recorded-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_nat    = cv_nat.get_feature_names()\n",
    "words_health = cv_health.get_feature_names()\n",
    "\n",
    "words_common = set(words_nat).intersection(words_health)\n",
    "words_common = list(words_common)\n",
    "# len(words_common)\n",
    "[words_nat.remove(word) for word in words_common];\n",
    "[words_health.remove(word) for word in words_common];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "operating-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run classification based on words that were exclusively used in Natural or Health condition\n",
    "\n",
    "n_words = len(words_nat)+len(words_health)\n",
    "n_words_nat = len(words_nat)\n",
    "X = np.full((n_words,300),np.nan) # 300 is spacy word vector length\n",
    "y = np.full((n_words),np.nan)\n",
    "for index,word in enumerate(words_nat):\n",
    "    X[index,:] = nlp(word).vector\n",
    "    y[index]   = 0\n",
    "for index,word in enumerate(words_health):\n",
    "    X[index+n_words_nat,:] = nlp(word).vector\n",
    "    y[index+n_words_nat]   = 1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "about-guide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.55      0.53        51\n",
      "         1.0       0.51      0.47      0.49        51\n",
      "\n",
      "    accuracy                           0.51       102\n",
      "   macro avg       0.51      0.51      0.51       102\n",
      "weighted avg       0.51      0.51      0.51       102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25)\n",
    "\n",
    "svm = LinearSVC(max_iter=5000)\n",
    "svm.fit(X_train,y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "worth-exclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55882353 0.51470588 0.47058824 0.58208955 0.55223881 0.50746269]\n",
      "0.5309847819724905\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(svm,X,y,cv=6)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-capital",
   "metadata": {},
   "source": [
    "#### run classification with each word in each reason as an observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "loose-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data\n",
    "words_nat    = cv_nat.get_feature_names()\n",
    "words_health = cv_health.get_feature_names()\n",
    "\n",
    "reasons_nat    = list()\n",
    "reasons_health = list()\n",
    "\n",
    "for s in range(df_Nat.shape[0]):\n",
    "    for n in range(1,21):\n",
    "        for r in range(1,5):\n",
    "            if isinstance(df_Nat[str(n)+'_Reason'+str(r)].iloc[s],str):\n",
    "                reasons_nat = reasons_nat + df_Nat[str(n)+'_Reason'+str(r)].iloc[s].split()\n",
    "for s in range(df_Health.shape[0]):\n",
    "    for n in range(1,21):\n",
    "        for r in range(1,5):\n",
    "            if isinstance(df_Health[str(n)+'_Reason'+str(r)].iloc[s],str):\n",
    "                reasons_health = reasons_health + df_Health[str(n)+'_Reason'+str(r)].iloc[s].split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eight-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "[reasons_nat.remove(word) for word in reasons_nat if nlp.vocab[word].is_stop]; #spacy.lang.en.STOP_WORDS]#\n",
    "[reasons_health.remove(word) for word in reasons_health if nlp.vocab[word].is_stop]; #spacy.lang.en.STOP_WORDS]#\n",
    "\n",
    "# # type(spacy.lang.en.STOP_WORDS)\n",
    "# # nlp.vocab['food'].is_stop\n",
    "# # 'food' in spacy.lang.en.STOP_WORDS\n",
    "# len(reasons_nat)\n",
    "# for word in reasons_nat: \n",
    "# #     print(f' {word} | {nlp.vocab[word].is_stop}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "induced-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input to classifier\n",
    "n_words = len(reasons_nat)+len(reasons_health)\n",
    "n_words_nat = len(reasons_nat)\n",
    "\n",
    "X = np.full((n_words,300),np.nan) # 300 is spacy word vector length\n",
    "y = np.full((n_words),np.nan)\n",
    "for index,word in enumerate(reasons_nat):\n",
    "    X[index,:] = nlp(word).vector\n",
    "    y[index]   = 0\n",
    "for index,word in enumerate(reasons_health):\n",
    "    X[index+n_words_nat,:] = nlp(word).vector\n",
    "    y[index+n_words_nat]   = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "minimal-courtesy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "parental-mounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.56      0.58      2565\n",
      "         1.0       0.66      0.68      0.67      3193\n",
      "\n",
      "    accuracy                           0.63      5758\n",
      "   macro avg       0.62      0.62      0.62      5758\n",
      "weighted avg       0.63      0.63      0.63      5758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25)\n",
    "\n",
    "svm = LinearSVC(max_iter=5000)\n",
    "svm.fit(X_train,y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "collect-eagle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57931753 0.65433707 0.56056265 0.55810318 0.62115685 0.582074  ]\n",
      "0.5925918787022724\n"
     ]
    }
   ],
   "source": [
    "# # use svm to classify\n",
    "svm = LinearSVC(max_iter=5000)\n",
    "scores = cross_val_score(svm,X,y,cv=6)\n",
    "print(scores)\n",
    "print(scores.mean())\n",
    "# svm.fit(X_train,y_train)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25)\n",
    "\n",
    "# svm = LinearSVC(max_iter=5000)\n",
    "# svm.fit(X_train,y_train)\n",
    "# y_pred = svm.predict(X_test)\n",
    "\n",
    "# print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-indonesia",
   "metadata": {},
   "source": [
    "#### run classification with each reason as an observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "disabled-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data\n",
    "words_nat    = cv_nat.get_feature_names()\n",
    "words_health = cv_health.get_feature_names()\n",
    "\n",
    "reasons_nat    = list()\n",
    "reasons_health = list()\n",
    "\n",
    "for s in range(df_Nat.shape[0]):\n",
    "    for n in range(1,21):\n",
    "        for r in range(1,5):\n",
    "            if isinstance(df_Nat[str(n)+'_Reason'+str(r)].iloc[s],str):\n",
    "                reasons_nat.append(df_Nat[str(n)+'_Reason'+str(r)].iloc[s]) \n",
    "for s in range(df_Health.shape[0]):\n",
    "    for n in range(1,21):\n",
    "        for r in range(1,5):\n",
    "            if isinstance(df_Health[str(n)+'_Reason'+str(r)].iloc[s],str):\n",
    "                reasons_health.append(df_Health[str(n)+'_Reason'+str(r)].iloc[s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "painted-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reasons_nat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "absolute-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "for ind,reason in enumerate(reasons_nat):\n",
    "    temp = reason.split()\n",
    "    [temp.remove(word) for word in temp if nlp.vocab[word].is_stop]; #spacy.lang.en.STOP_WORDS]#\n",
    "    reasons_nat[ind] = ' '.join(temp)\n",
    "\n",
    "for ind,reason in enumerate(reasons_health):\n",
    "    temp = reason.split()\n",
    "    [temp.remove(word) for word in temp if nlp.vocab[word].is_stop]; #spacy.lang.en.STOP_WORDS]#\n",
    "    reasons_health[ind] = ' '.join(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "headed-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reasons_nat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "universal-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input to classifier\n",
    "n_words = len(reasons_nat)+len(reasons_health)\n",
    "n_words_nat = len(reasons_nat)\n",
    "\n",
    "X = np.full((n_words,300),np.nan) # 300 is spacy word vector length\n",
    "y = np.full((n_words),np.nan)\n",
    "for index,reason in enumerate(reasons_nat):\n",
    "    X[index,:] = nlp(reason).vector\n",
    "    y[index]   = 0\n",
    "    \n",
    "for index,reason in enumerate(reasons_health):\n",
    "    X[index+n_words_nat,:] = nlp(reason).vector\n",
    "    y[index+n_words_nat]   = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "lightweight-eagle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.69      0.68       733\n",
      "         1.0       0.68      0.67      0.67       725\n",
      "\n",
      "    accuracy                           0.68      1458\n",
      "   macro avg       0.68      0.68      0.68      1458\n",
      "weighted avg       0.68      0.68      0.68      1458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25)\n",
    "\n",
    "svm = LinearSVC(max_iter=5000)\n",
    "svm.fit(X_train,y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "surrounded-salem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6069317830245854\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(max_iter=5000)\n",
    "scores = cross_val_score(svm,X,y,cv=6)  \n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will take a long time to run on PC\n",
    "svm = LinearSVC(max_iter=5000)\n",
    "scores = cross_val_score(svm,X,y,cv=X.shape[0],n_jobs=-1) # leave-one-out  \n",
    "print(scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
